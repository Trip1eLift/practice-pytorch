{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is Transformer used in translation\n",
    "\n",
    "## Translating \"Hello World\" to Chinese\n",
    "\n",
    "### Stage 1\n",
    "1. Pass \"Hello World\" to encoder\n",
    "2. Pass \"<start>\" to decoder\n",
    "3. Transformer outputs \"你\"\n",
    "\n",
    "### Stage 2\n",
    "1. Pass \"Hello World\" to encoder\n",
    "2. Pass \"你\" to decoder\n",
    "3. Transformer outputs \"好\"\n",
    "\n",
    "### Stage 3\n",
    "1. Pass \"Hello World\" to encoder\n",
    "2. Pass \"好\" to decoder\n",
    "3. Transformer outputs \"世\"\n",
    "\n",
    "### Stage 4\n",
    "1. Pass \"Hello World\" to encoder\n",
    "2. Pass \"世\" to decoder\n",
    "3. Transformer outputs \"界\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 'name' embedded and pos encoded into 512x1 vector.\n",
      "512x1 vector is pass to Q, K, V as 3 linear mapped copies.\n",
      "In 8 heads attentions architecture, 512x1 vector is chopped into 8 pieces with 64x1 each.\n",
      "This is done alone with other words in the sentence.\n",
      "\n",
      "Say hello to world\n",
      "embeded and pos encoded:             4x512\n",
      "Linear maps to Q K V:              3x4x512\n",
      "Attention unit starts\n",
      "In Q:                                4x512\n",
      "8 heads chopping:                     4x64\n",
      "Attention takes Q K V of 4x64:      3x4x64\n",
      "Attention generates new V:            4x64\n",
      "Attention generates Attn matrix:       4x4\n",
      "Concat all 8 new_V from 8 Attention: 4x512\n",
      "Goes to next layer...\n"
     ]
    }
   ],
   "source": [
    "# Multi-head Attention\n",
    "\n",
    "print(\"word 'name' embedded and pos encoded into 512x1 vector.\")\n",
    "print(\"512x1 vector is pass to Q, K, V as 3 linear mapped copies.\")\n",
    "print(\"In 8 heads attentions architecture, 512x1 vector is chopped into 8 pieces with 64x1 each.\")\n",
    "print(\"This is done alone with other words in the sentence.\\n\")\n",
    "\n",
    "print(\"Say hello to world\")\n",
    "print(\"embeded and pos encoded:             4x512\")\n",
    "print(\"Linear maps to Q K V:              3x4x512\")\n",
    "print(\"Attention unit starts\")\n",
    "print(\"In Q:                                4x512\")\n",
    "print(\"8 heads chopping:                     4x64\")\n",
    "print(\"Attention takes Q K V of 4x64:      3x4x64\")\n",
    "print(\"Attention generates new V:            4x64\")\n",
    "print(\"Attention generates Attn matrix:       4x4\")\n",
    "print(\"Concat all 8 new_V from 8 Attention: 4x512\")\n",
    "print(\"Goes to next layer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sequence_length = 4 # sentence length\n",
    "batch_size = 1      # for parallel processing\n",
    "input_dim = 512     # dim of word after embedding and pos encoding\n",
    "d_model = 512       # output dim of multi head attn\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maps sequence to Q, K, V\n",
    "qkv_layer = nn.Linear(input_dim, 3 * d_model) # Maps 512 to 3*512=1536 as QKV\n",
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) * 3\n",
    "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # 512 / 8 = 64\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim) # 3 is from QKV\n",
    "qkv.shape\n",
    "# 192 = 64 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate Q K V\n",
    "q, k, v = qkv.chunk(3, dim=-1) # chop the last dimension (-1) into 3\n",
    "(q.shape, k.shape, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention matrix\n",
    "d_k = q.size()[-1] # should be 64=512/8 in our case\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # transpose between the second last (-2) and the last (-1) dimension\n",
    "scaled.shape\n",
    "# focus on the last 2 dim\n",
    "# 4x64 dot 4x64^T = 4x4 attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[-0.4880,    -inf,    -inf,    -inf],\n",
      "        [-0.2902, -0.0770,    -inf,    -inf],\n",
      "        [ 0.0404,  0.3003,  0.0707,    -inf],\n",
      "        [ 0.1488, -0.2462,  0.5967,  0.4996]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1) # tri will auto locate the last two dimension\n",
    "print(mask[0][0])\n",
    "print(mask[0][1])\n",
    "print((scaled + mask)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4469, 0.5531, 0.0000, 0.0000],\n",
       "        [0.3005, 0.3897, 0.3098, 0.0000],\n",
       "        [0.2146, 0.1446, 0.3359, 0.3048]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax and mask\n",
    "scaled += mask\n",
    "attention = F.softmax(scaled, dim=-1) # apply softmax on last dimension which is row by row [1, 8, 4, 4]\n",
    "attention[0][0] # softmax: [1][1] of the 4x4 attention will equal exp(scaled[1][0]) / (exp(scaled[1][0])+exp(scaled[1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = torch.matmul(attention, v)\n",
    "new_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own implementation of multi headed attention\n",
    "\n",
    "def multi_head_scaled_dot_product(Q, K, V, num_heads=8, mask=None):\n",
    "    # Assume no batch here Q.shape -> [4, 512]\n",
    "    # Also, 512 is after the word goes through the Linear mapping: input_dim -> d_model\n",
    "    # Q, K, V can be 4x512 (4 words long sequence)\n",
    "    d_model = Q.size()[-1]          # 512\n",
    "    sequence_length = Q.size()[-2]  # 4\n",
    "\n",
    "    # split heads\n",
    "    head_dim = d_model // num_heads # 512 / 8 = 64\n",
    "    Q = Q.reshape(sequence_length, num_heads, head_dim).permute(1, 0, 2) # [num_heads, sequence_length, head_dim]\n",
    "    K = K.reshape(sequence_length, num_heads, head_dim).permute(1, 0, 2)\n",
    "    V = V.reshape(sequence_length, num_heads, head_dim).permute(1, 0, 2)\n",
    "\n",
    "    new_V = torch.tensor([[] for n in range(sequence_length)])\n",
    "    Attn = torch.empty(0, sequence_length, sequence_length)\n",
    "    # this part can be parallelize\n",
    "    for q_h, k_h, v_h in zip(Q, K, V):\n",
    "        # q_h.shape = [4, 64]\n",
    "        d_k = q_h.size()[-1] # 64\n",
    "        scaled = torch.matmul(q_h, k_h.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled += mask\n",
    "\n",
    "        attn = F.softmax(scaled, dim=-1) # apply softmax row by row. attn: 4x4\n",
    "        new_v = torch.matmul(attn, v_h)  # 4x64 -> needs to concat with other heads to form 4x512\n",
    "        # do you add attn up for multi head attn return?\n",
    "        new_V = torch.cat((new_V, new_v), -1) # 4x64 -> 4x512\n",
    "        attn = attn.reshape(1, sequence_length, sequence_length)\n",
    "        Attn = torch.cat((Attn, attn), 0) # 4x4 -> 8x4x4\n",
    "\n",
    "    return new_V, Attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better implementation of multi headed attention\n",
    "import copy\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) # maps value vector to output\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f'x.size(): {x.size()}')\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f'qkv.size(): {qkv.size()}')\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f'qkv.size(): {qkv.size()}')\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        self.qkv = qkv.clone().detach() # for comparision\n",
    "        print(f'qkv.size(): {qkv.size()}')\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
    "        self.values = values.clone().detach() # for comparision\n",
    "        self.attention = attention.clone().detach() # for comparision\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled += mask\n",
    "        attention = F.softmax(scaled, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "    def get_qkv(self):\n",
    "        return self.qkv.chunk(3, dim=-1)\n",
    "    \n",
    "    def get_value_attention(self):\n",
    "        return self.values, self.attention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "q size: torch.Size([5, 512]), k size: torch.Size([5, 512]), v size: torch.Size([5, 512]), \n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([0.2224, 0.0200, 0.6099, 0.0143])\n",
      "tensor([0.2224, 0.0200, 0.6099, 0.0143])\n",
      "Congrads! My own implementation of Multi Headed Attention is correct.\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)\n",
    "\n",
    "# qkv, new_v, attention from model\n",
    "q, k, v = model.get_qkv()\n",
    "M_value, M_attention = model.get_value_attention()\n",
    "M_value = M_value[0]\n",
    "M_attention = M_attention[0]\n",
    "\n",
    "q = q[0].permute(1, 0, 2).reshape(sequence_length, d_model)\n",
    "k = k[0].permute(1, 0, 2).reshape(sequence_length, d_model)\n",
    "v = v[0].permute(1, 0, 2).reshape(sequence_length, d_model)\n",
    "\n",
    "print()\n",
    "print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "\n",
    "# new_v, attention from my own implementation\n",
    "values, attention = multi_head_scaled_dot_product(q, k, v, num_heads=num_heads)\n",
    "\n",
    "M_value = M_value.permute(1, 0, 2).reshape(sequence_length, d_model)\n",
    "print(M_value.shape)\n",
    "print(values.shape)\n",
    "print(M_value[0][0:4])\n",
    "print(values[0][0:4])\n",
    "print(\"Congrads! My own implementation of Multi Headed Attention is correct.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d470c6f7045cb7376d33eb71b881a804349e7225012e3e94a4e01aa7b0ebf46e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
